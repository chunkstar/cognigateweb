# ğŸ” Preview Guide - Try Cognigate Locally

The library is **fully built and ready to use**! Here are several ways to preview it:

---

## âœ… Quick Verification (No API Key Needed)

Just built and verified successfully:

```bash
node preview-test.js
```

**Output:**
```
ğŸš€ Cognigate Preview Test

âœ… Test 1: Can create gateway
   Gateway created successfully!

âœ… Test 2: Budget tracking works
   Daily limit: $10.00
   Used: $0.0000
   Remaining: $10.00
   Resets: [time]

âœ… Test 3: Configuration accessible
   Cache enabled: true
   Compression: medium
   Local fallback: true

âœ… Test 4: All methods available
   ai.complete: function
   ai.stream: function
   ai.getBudgetStatus: function
   ai.clearCache: function

ğŸ‰ All preview tests passed!
```

---

## ğŸ® Interactive Preview (With API Key)

### Option 1: Basic Chat (Recommended)

```bash
# Set your API key
export OPENAI_API_KEY="sk-your-key-here"

# Run the example
npx tsx examples/basic-chat.ts
```

**What you'll see:**
- 3 questions answered by AI
- Budget tracking after each request
- Cost breakdown
- Reset time

### Option 2: Budget-Aware App

```bash
export OPENAI_API_KEY="sk-your-key-here"

npx tsx examples/budget-aware-app.ts
```

**Features:**
- Aggressive cost optimization
- Semantic caching demo
- Budget alerts
- Fallback behavior

### Option 3: Streaming Chat

```bash
export OPENAI_API_KEY="sk-your-key-here"

npx tsx examples/streaming-chat.ts
```

**Features:**
- Real-time token streaming
- Live response generation
- Performance metrics

---

## ğŸ§ª Test with Local Models (100% Free)

No API key needed! Uses Ollama/LM Studio:

```bash
# Install Ollama (if not installed)
# macOS: brew install ollama
# Windows: Download from ollama.ai

# Start Ollama
ollama serve

# In another terminal
ollama pull llama2

# Now run with local-only mode
npx tsx -e "
import { createGateway } from './dist/index.js';

const ai = createGateway({
  dailyBudget: 0,  // Unlimited (it's free!)
  localFallback: {
    enabled: true,
    providers: ['ollama']
  }
});

const answer = await ai.complete('What is TypeScript?', {
  forceProvider: 'local'
});

console.log('Answer:', answer);
"
```

---

## ğŸ“¦ Use in Your Own Code

### Create a test file:

**test-cognigate.js:**
```javascript
import { createGateway } from './dist/index.js';

const ai = createGateway({
  dailyBudget: 5.00,
  cacheEnabled: true,
  cloudProviders: {
    openai: {
      apiKey: process.env.OPENAI_API_KEY
    }
  }
});

// Ask a question
const answer = await ai.complete("Explain async/await in one sentence");
console.log('Answer:', answer);

// Check budget
const status = ai.getBudgetStatus();
console.log(`\nCost: $${status.used.toFixed(6)}`);
```

**Run it:**
```bash
export OPENAI_API_KEY="sk-..."
node test-cognigate.js
```

---

## ğŸŒ Browser Preview

### Create an HTML file:

**preview.html:**
```html
<!DOCTYPE html>
<html>
<head>
  <title>Cognigate Preview</title>
</head>
<body>
  <h1>ğŸ¤– Cognigate Preview</h1>
  <button id="askBtn">Ask AI</button>
  <div id="result"></div>

  <script type="module">
    import { createGateway } from './dist/index.js';

    const ai = createGateway({
      dailyBudget: 0,  // Unlimited for demo
      localFallback: { enabled: true }
    });

    document.getElementById('askBtn').onclick = async () => {
      document.getElementById('result').textContent = 'Thinking...';

      try {
        const answer = await ai.complete("Tell me a joke");
        document.getElementById('result').textContent = answer;

        const status = ai.getBudgetStatus();
        console.log('Budget:', status);
      } catch (error) {
        document.getElementById('result').textContent =
          'Error: ' + error.message + ' (Try setting up a provider)';
      }
    };
  </script>
</body>
</html>
```

**Run it:**
```bash
# Simple HTTP server
npx serve .
# or
python -m http.server 8000

# Open: http://localhost:8000/preview.html
```

---

## ğŸ§‘â€ğŸ’» Run All Tests

See everything working:

```bash
npm test
```

**Expected output:**
```
âœ“ tests/core/compressor.test.ts (31 tests)
âœ“ tests/core/semantic-cache.test.ts (22 tests)
âœ“ tests/core/gateway.test.ts (47 tests)
âœ“ tests/webhooks/webhook-manager.test.ts (15 tests)
âœ“ tests/core/alert-manager.test.ts (15 tests)
âœ“ tests/core/stream.test.ts (13 tests)
... (296 total tests)

Test Files: 14 passed (14)
Tests: 296 passed (296)
```

---

## ğŸ”¥ Quick API Test

Without any files:

```bash
npx tsx -e "
import { createGateway } from './dist/index.js';

console.log('Testing Cognigate...\n');

const ai = createGateway({
  dailyBudget: 10,
  cacheEnabled: true
});

console.log('âœ… Gateway created');
console.log('âœ… Budget:', ai.getBudgetStatus().dailyLimit);
console.log('âœ… All methods available');
console.log('\nğŸ’¡ Set OPENAI_API_KEY to test AI completions');
"
```

---

## ğŸ“Š What's Working

| Feature | Status | Test Command |
|---------|--------|--------------|
| **Build** | âœ… Working | `npm run build` |
| **Tests** | âœ… 296 passing | `npm test` |
| **Budget Tracking** | âœ… Working | `node preview-test.js` |
| **Caching** | âœ… Working | Tests verify |
| **Compression** | âœ… Working | Tests verify |
| **Streaming** | âœ… Working | `npx tsx examples/streaming-chat.ts` |
| **Voice** | âœ… Working | Browser required |
| **Webhooks** | âœ… Working | Tests verify |
| **Multiple Providers** | âœ… Working | Tests verify |
| **Local Fallback** | âœ… Working | With Ollama |

---

## ğŸ¯ Recommended Preview Flow

1. **Verify basics** (no API key):
   ```bash
   node preview-test.js
   ```

2. **Run tests** (see it work):
   ```bash
   npm test
   ```

3. **Try with real API**:
   ```bash
   export OPENAI_API_KEY="sk-..."
   npx tsx examples/basic-chat.ts
   ```

4. **Or use local models** (free):
   ```bash
   ollama serve  # in one terminal
   ollama pull llama2
   # Use forceProvider: 'local' in code
   ```

---

## â“ Troubleshooting

### "Cannot find module"
```bash
npm run build  # Rebuild dist/
```

### "API key not found"
```bash
export OPENAI_API_KEY="sk-..."
# Or create .env file
```

### "tsx not found"
```bash
npm install -g tsx
# Or use npx tsx
```

---

## ğŸš€ Ready to Use!

The library is **100% functional** and ready for:
- âœ… Local testing
- âœ… Development
- âœ… Production use
- âœ… npm publishing

**Next steps:**
1. Try the examples above
2. Build your own app
3. Publish to npm (optional)

---

Last updated: November 22, 2025
Version: 1.0.0
